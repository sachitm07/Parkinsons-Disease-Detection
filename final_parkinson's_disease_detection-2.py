# -*- coding: utf-8 -*-
"""Final Parkinson's Disease Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KZW3kDOnEEbREmW5bmL0JVPQGuUtbImM

Importing the Dependencies
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn import svm
from sklearn.metrics import accuracy_score

"""Data Collection & Analysis"""

# loading the data from csv file to a Pandas DataFrame
parkinsons_data = pd.read_csv('/content/parkinsons.csv')

# printing the first 5 rows of the dataframe
parkinsons_data.head()

# number of rows and columns in the dataframe
parkinsons_data.shape

# getting more information about the dataset
parkinsons_data.info()

# checking for missing values in each column
parkinsons_data.isnull().sum()

# getting some statistical measures about the data
parkinsons_data.describe()

# distribution of target Variable
parkinsons_data['status'].value_counts()

"""1  --> Parkinson's Positive

0 --> Healthy

"""

# grouping the data bas3ed on the target variable
parkinsons_data.groupby('status').mean()

"""Data Pre-Processing

Separating the features & Target
"""

X = parkinsons_data.drop(columns=['name','status'], axis=1)
Y = parkinsons_data['status']

print(X)

print(Y)

"""Splitting the data to training data & Test data"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)

print(X.shape, X_train.shape, X_test.shape)

"""Data Standardization"""

scaler = StandardScaler()

scaler.fit(X_train)

X_train = scaler.transform(X_train)

X_test = scaler.transform(X_test)

print(X_train)

"""Model Training

Support Vector Machine Model
"""

model = svm.SVC(kernel='linear')

# training the SVM model with training data
model.fit(X_train, Y_train)

"""Model Evaluation

Accuracy Score
"""

# accuracy score on training data
X_train_prediction = model.predict(X_train)
training_data_accuracy = accuracy_score(Y_train, X_train_prediction)

print('Accuracy score of training data : ', training_data_accuracy)

# accuracy score on training data
X_test_prediction = model.predict(X_test)
test_data_accuracy = accuracy_score(Y_test, X_test_prediction)

print('Accuracy score of test data : ', test_data_accuracy)

"""Building a Predictive System"""

input_data = (197.07600,206.89600,192.05500,0.00289,0.00001,0.00166,0.00168,0.00498,0.01098,0.09700,0.00563,0.00680,0.00802,0.01689,0.00339,26.77500,0.422229,0.741367,-7.348300,0.177551,1.743867,0.085569)

# changing input data to a numpy array
input_data_as_numpy_array = np.asarray(input_data)

# reshape the numpy array
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

# standardize the data
std_data = scaler.transform(input_data_reshaped)

prediction = model.predict(std_data)
print(prediction)


if (prediction[0] == 0):
  print("The Person does not have Parkinsons Disease")

else:
  print("The Person has Parkinsons")

column_names = list(parkinsons_data.columns.values)
print(column_names)
print(len(X))

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn import svm
from sklearn.preprocessing import StandardScaler

# Standardize the entire dataset for consistent visualization
scaler = StandardScaler()
X_std = scaler.fit_transform(X)

# Perform PCA to reduce the features to a lower dimension (e.g., 2D for visualization)
pca = PCA(n_components=2)
X_2D = pca.fit_transform(X_std)

# Train an SVM model on the reduced feature space
model_2D = svm.SVC(kernel='linear')
model_2D.fit(X_2D, Y)

# Scatter plot the data points
plt.scatter(X_2D[:, 0], X_2D[:, 1], c=Y, cmap=plt.cm.Paired)

# Plot the decision boundary
ax = plt.gca()
xlim = ax.get_xlim()
ylim = ax.get_ylim()

xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50), np.linspace(ylim[0], ylim[1], 50))
Z = model_2D.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Plot the decision boundary and margins
ax.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])

# Highlight the support vectors
ax.scatter(model_2D.support_vectors_[:, 0], model_2D.support_vectors_[:, 1], s=100, facecolors='none', edgecolors='k')

plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend(['Class 0', 'Class 1', 'Support Vectors'])
plt.title('SVM Decision Boundary (2D Reduced Feature Space)')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn import svm
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score, KFold

# Standardize the entire dataset for consistent visualization
scaler = StandardScaler()
X_std = scaler.fit_transform(X)

# Perform PCA to reduce the features to a lower dimension (e.g., 2D for visualization)
pca = PCA(n_components=2)
X_2D = pca.fit_transform(X_std)

# Train an SVM model on the reduced feature space
model_2D = svm.SVC(kernel='rbf', C=1, gamma=0.1)
model_2D.fit(X_2D, Y)

# Create a mesh grid for the contour map
xx, yy = np.meshgrid(np.linspace(X_2D[:, 0].min() - 1, X_2D[:, 0].max() + 1, 100),
                     np.linspace(X_2D[:, 1].min() - 1, X_2D[:, 1].max() + 1, 100))

# Predict class labels for each point in the mesh grid
Z = model_2D.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Create a contour map
plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)

# Scatter plot the data points
plt.scatter(X_2D[:, 0], X_2D[:, 1], c=Y, cmap=plt.cm.Paired, edgecolors='k')

# Highlight the support vectors
plt.scatter(model_2D.support_vectors_[:, 0], model_2D.support_vectors_[:, 1], s=100, facecolors='none', edgecolors='k')

# Plot the decision boundary lines
plt.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])

plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend(['Class 0', 'Class 1', 'Support Vectors'])
plt.title('SVM Decision Boundary (2D Reduced Feature Space) - Contour Map with Decision Boundary')
plt.show()

num_folds = 5
kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)

# Perform k-fold cross-validation
scores = cross_val_score(model, X_std, Y, cv=kfold, scoring='accuracy')

# Print the cross-validation results
print("Cross-Validation Scores:", scores)
print("Mean Accuracy:", np.mean(scores))
print("Standard Deviation of Accuracy:", np.std(scores))

